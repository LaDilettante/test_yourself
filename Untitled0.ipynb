{
 "metadata": {
  "name": "",
  "signature": "sha256:2d4ab26f5ac46c4f9ac570df8dfe1b37becd8e876c9a611ff5bdd8384c7e7e86"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Singular value decomposition & Eigendecomposition:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given the singular value decomposition (SVD) of any matrix A $(m \\times n)$\n",
      "\n",
      "$$A = USV'$$\n",
      "\n",
      "prove that:\n",
      "\n",
      "$U$ is $m \\times m$, whose columns are the eigenvectors of $AA'$ (also the basis of the output space of A)\n",
      "\n",
      "$S = diag(s)$ with $s$ being the singular values of A\n",
      "\n",
      "$V$ is $n \\times n$, whose columns are the eigenvectors of $A'A$ (also the basis of the input space of A)\n",
      "\n",
      "[Solution](#svd_eigendecomposition-solution)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "The eigenvector associated with the largest eigenvalues is the direction in which maximize variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's frame PCA as choosing a new basis for the data in a way that preserve the most variance. \n",
      "\n",
      "Given the matrix $\\mathbf{x} = \\{x_1, \\dots, x_n\\}$ in $R^{m \\times n}$, the covariance matrix is $\\Sigma = \\mathbf{x}\\mathbf{x}'$ in $R^{m \\times m}$\n",
      "\n",
      "Now let $u \\in R^m$ be a new basis to project our data on. Then the new, projected data matrix will be $u'\\mathbf{x} = \\{u'x_1, \\dots, u'x_n\\}$. The covariance matrix of this projected data matrix is $u'\\mathbf{x} (u'\\mathbf{x})' = u' \\mathbf{x}\\mathbf{x}' u = u'\\Sigma u$\n",
      "\n",
      "We want to find $u$ such that $u'\\Sigma u$ is maximized, i.e. the projected data captures the most variance. This is a standard constrained optimization problem, with the constraint that $|u| = 1$ since $u$ is a basis vector.\n",
      "\n",
      "Show that when we solve this constrained optimization problem, it turns out that $u$ is the eigenvector of $\\Sigma$ associated with the largest eigenvalue.\n",
      "\n",
      "[Solution](#first_eigenvector_maximizes_variance-solution)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a name=\"first_eigenvector_maximizes_variance-solution\"></a>\n",
      "# Solution\n",
      "\n",
      "The constrained optimization is:\n",
      "\n",
      "$$u^* = \\mathrm{argmax} u'\\Sigma u \\qquad \\text{s.t. } |u| = 1 \\qquad \\text{or } |u|^2 = u'u = 1$$\n",
      "\n",
      "Set up as Lagrangian optimization:\n",
      "\n",
      "$$\\mathcal{L} = u' \\Sigma u - \\alpha (u'u - 1)$$\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{\\partial}{\\partial u} \\mathcal{L} = 2u'\\Sigma - 2\\alpha u' &= 0 \\\\\n",
      "u'\\Sigma &= \\alpha u' \\\\\n",
      "\\Sigma u &= \\alpha u & \\text{Tranpose both sides, with $\\Sigma$ being symmetric} \\\\\n",
      "\\frac{\\partial}{\\partial \\alpha} \\mathcal{L} = u'u -1 &= 0 & \\\\\n",
      "u'u &= 1 &\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since $\\Sigma u = \\alpha u$, we now know that $u$ is an eigenvector of $\\Sigma$ with the associated eigenvalue $\\alpha$. But which eigenvector / eigenvalue pair do we choose to maximize $u'\\Sigma u$?\n",
      "\n",
      "Note that: $u' \\Sigma u = \\alpha u' u = \\alpha$, so we want to choose the largest $\\alpha$ possible.\n",
      "\n",
      "Therefore, we want to pick $\\alpha$ as the largest eigenvalue of $\\Sigma$, and $u$ as the eigenvector associated with it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Newton conjugate gradient method: https://www.rose-hulman.edu/~bryan/lottamath/congrad.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a name=\"svd_eigendecomposition-solution\"></a>\n",
      "# Solution\n",
      "$$\n",
      "\\begin{align}\n",
      "A &= USV' \\\\\n",
      "AA' &= (USV')(USV')' \\\\\n",
      "&= USV' VS'U \\\\\n",
      "&= US^2U & \\text{Since $S$ is diagonal, $S' = S$} \\\\\n",
      "A'A &= (USV')'(USV') \\\\\n",
      "&= VSU'USV' \\\\\n",
      "&= VS^2V' & \\text{Since $U$ is a orthogonal matrix, $U' = U^{-1}$}\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "Note that to do PCA, we want to do eigendecomposition on the covariance matrix, i.e. $AA' / (n-1)$ (given that the data matrix A has mean 0).\n",
      "\n",
      "With the above result, instead of doing a eigendecomposition on $AA'$, we can do SVD on $A$. Then, $U$ will be the eigenvectors (principal components) and $S^2 / (n-1)$ will be the eigenvalues."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}